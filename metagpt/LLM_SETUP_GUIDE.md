# üöÄ LLM Setup Guide - Solve Timeout Issues

## ‚ö° **Best Solutions for Timeout Problems:**

### **Option 1: OpenAI API (Fastest, Paid)**
- **Speed:** ~2-5 seconds per request
- **Quality:** Excellent 
- **Cost:** ~$0.01-0.03 per request

```bash
# Setup
pip install openai
export OPENAI_API_KEY="your-api-key"

# Test
python metagpt_multi_llm.py speed-test
python metagpt_multi_llm.py todo
```

### **Option 2: Anthropic API (Fast, Paid)**
- **Speed:** ~3-8 seconds per request
- **Quality:** Excellent
- **Cost:** ~$0.015-0.075 per request

```bash
# Setup  
pip install anthropic
export ANTHROPIC_API_KEY="your-api-key"

# Test
python metagpt_multi_llm.py speed-test
```

### **Option 3: Ollama (Free, Local)**
- **Speed:** ~10-30 seconds per request
- **Quality:** Good
- **Cost:** Free

```bash
# Setup
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama2
ollama serve

# Test
python metagpt_multi_llm.py speed-test
```

### **Option 4: Improved Claude Code (Fallback)**
- **Speed:** ~60-180 seconds per request
- **Quality:** Excellent
- **Cost:** Free (requires Claude Code subscription)

## üìä **Performance Comparison:**

| Provider | Speed | Quality | Cost | Setup |
|----------|-------|---------|------|-------|
| OpenAI | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞üí∞ | Easy |
| Anthropic | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞üí∞ | Easy |
| Ollama | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Free | Medium |
| Claude Code | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Free* | Easy |

## üéØ **Recommended Setup:**

### For Maximum Speed:
```bash
# Get OpenAI API key from https://platform.openai.com/
export OPENAI_API_KEY="sk-..."

# Run fast project
python metagpt_multi_llm.py todo
# Completes in ~2-3 minutes instead of 15+ minutes
```

### For Free Solution:
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama2
ollama serve

# Run project
python metagpt_multi_llm.py todo
# Completes in ~5-8 minutes
```

### For Hybrid (Best of Both):
```bash
# Setup both
export OPENAI_API_KEY="sk-..."
ollama serve

# System automatically uses fastest available
python metagpt_multi_llm.py todo
```

## üîß **Technical Improvements Made:**

### 1. **Timeout Management**
```python
# Before: 60-180s timeout with frequent failures
# After: 30-120s timeout with fallback providers

try:
    result = await asyncio.wait_for(
        provider.acompletion_text(messages),
        timeout=120  # 2 minutes max
    )
except asyncio.TimeoutError:
    # Automatically try next provider
```

### 2. **Concurrent Execution**
```python
# Before: Sequential execution (slow)
# After: Parallel where possible

pm_task = self._run_requirements(description)
arch_task = self._run_architecture(description)
pm_result, arch_result = await asyncio.gather(pm_task, arch_task)
```

### 3. **Smart Provider Selection**
```python
# Automatically chooses fastest available:
# 1. OpenAI (if API key available)
# 2. Anthropic (if API key available) 
# 3. Ollama (if running locally)
# 4. Claude Code (fallback)
```

### 4. **Response Length Limits**
```python
# Prevents long responses that cause timeouts
"Keep response focused and under 800 words."
"Limit to 500 words max."
```

## üöÄ **Usage Examples:**

### Quick Test:
```bash
python metagpt_multi_llm.py speed-test
```

### Fast Todo App:
```bash
python metagpt_multi_llm.py todo
# Output: fast_todo_20240819_092000/
```

### Custom Project:
```bash
python metagpt_multi_llm.py custom "Create a weather app with location search"
```

### API vs Local Comparison:
```bash
# With API (2-3 minutes):
OPENAI_API_KEY="sk-..." python metagpt_multi_llm.py todo

# With local Ollama (5-8 minutes):
ollama serve
python metagpt_multi_llm.py todo

# With Claude Code only (10-15 minutes):
python metagpt_improved.py todo
```

## üí° **Pro Tips:**

### 1. **For Development:**
Use OpenAI API for fastest iteration during development.

### 2. **For Production:**
Use Ollama for cost-effective deployment.

### 3. **For Experiments:**
Use fallback system to ensure projects always complete.

### 4. **For Large Projects:**
Break into smaller components and run multiple sessions.

## üî• **Expected Speed Improvements:**

| Task | Claude Code Only | With OpenAI API | Improvement |
|------|------------------|-----------------|-------------|
| Simple todo | 15-20 min | 2-3 min | **5-7x faster** |
| Complex webapp | 30-45 min | 5-8 min | **6-9x faster** |
| Step completion | 2-5 min | 10-30 sec | **4-10x faster** |

## ‚ö†Ô∏è **Troubleshooting:**

### OpenAI API Issues:
```bash
# Check API key
echo $OPENAI_API_KEY

# Test connection
pip install openai
python -c "import openai; print('OpenAI available')"
```

### Ollama Issues:
```bash
# Check if running
curl http://localhost:11434/api/tags

# Start service
ollama serve

# Install model
ollama pull llama2
```

### Claude Code Issues:
```bash
# Check availability
claude --help

# Update timeout in code
ClaudeCodeProvider(timeout=300)  # 5 minutes
```

The multi-LLM approach solves the timeout problem by providing **faster, more reliable alternatives** while keeping Claude Code as a fallback! üéâ